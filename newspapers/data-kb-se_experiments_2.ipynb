{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the KB API:s for newspapers\n",
    "Goal: Get links to all jp2-files. They are contained in \"dark-packages\" so we need to get these first.\n",
    "\n",
    "Results: This code is used to produce (aftonbladet|svd|dn).csv\n",
    "\n",
    "TODO: after KBs SSL certificate seemed to expire this code won't work until requests are rewritten like this:\n",
    "```\n",
    "# !? had to disable verify certificate, something strange going on... !?\n",
    "    requests.packages.urllib3.disable_warnings()\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "```\n",
    "\n",
    "#### GET known dark-package with all files (Norrköpings W)\n",
    "```bash\n",
    "curl -X 'GET' \\\n",
    "  'https://data.kb.se/dark-8428195' \\\n",
    "  -H 'accept: application/json'\n",
    "\n",
    "```\n",
    "#### GET Aftonbladet whole dataset\n",
    "```bash\n",
    "curl -X 'GET' \\\n",
    "  'https://data.kb.se/dataset/7mf87n2g5xzjf8r4' \\\n",
    "  -H 'accept: application/json'\n",
    "\n",
    "```\n",
    "\n",
    "This gets the metadata post for the dataset. But it is unusable, as it doesn't contains any links to dark-packages which are the api-objects that contain actual links to image files. For an alternative approach using the search API, see below.\n",
    "\n",
    "#### Norrköpings Weckobladh\n",
    "\n",
    "NOTE: The pages in the dataset browser are not sorted chronologically.\n",
    "\n",
    "First page url, offset=0: https://data.kb.se/dataset/m1tnm4cpktxgf28n?offset=0\n",
    "\n",
    "Final page url, offset=9990: https://data.kb.se/dataset/m1tnm4cpktxgf28n?offset=9990\n",
    "\n",
    "Number of dark-packages: 9990+10(?)\n",
    "\n",
    "NOTE: Seems like a very strange coincidence that the total number of dark-packages should be 10000 exactly. 1 dark-package is one number/issue.\n",
    "\n",
    "#### Aftonbladet\n",
    "\n",
    "First page url, offset=0: https://data.kb.se/dataset/7mf87n2g5xzjf8r4?offset=0\n",
    "\n",
    "Final page url, offset=9990: https://data.kb.se/dataset/7mf87n2g5xzjf8r4?offset=9990\n",
    "\n",
    "NOTE: Yep. Aftonbladet cuts off at 10000 too. That is not a coincidence, but arbitrary. The whole datasets have not been made available.\n",
    "\n",
    "\n",
    "#### Get metadata containing 10000 (arbitrary maximum) dark-package objects\n",
    "\n",
    "```bash\n",
    "curl -X 'GET' \\\n",
    "  'https://data.kb.se/search/?q=%2A&%40type=package&inDataset=https%3A%2F%2Fdata.kb.se%2Fdataset%2F7mf87n2g5xzjf8r4&searchGranularity=package&_sort=title&limit=10000&offset=0' \\\n",
    "  -H 'accept: application/json'\n",
    "```\n",
    "\n",
    "This gets \"all\" 10000 dark-packages in one json-object. Of course this is not the whole dataset though...\n",
    "The response actually contains \"total\": 10000 which confirms that KB has put in a 10k hardlimit for queries. Unbelievable. The API/web interface was completely unusable in practice before, but this makes it truly unusable in theory as well.\n",
    "\n",
    "#### id:s for all newspaper-datasets\n",
    "\n",
    "```yaml\n",
    "\"@id\": \"https://data.kb.se/dataset/m1tnm4cpktxgf28n\"\n",
    "\"@type\": \"Dataset\"\n",
    "\"title\": \"Norrköpings tidningar 1787-1895\"\n",
    "\n",
    "\"@id\": \"https://data.kb.se/dataset/6ld76n1p44s12ht1\"\n",
    "\"@type\": \"Dataset\"\n",
    "\"title\"  \"Dagens nyheter\"\n",
    "\n",
    "\"@id\": \"https://data.kb.se/dataset/7mf87n2g5xzjf8r4\"\n",
    "\"@type\": \"Dataset\"\n",
    "\"title\": \"Aftonbladet\"\n",
    "\n",
    "\"@id\": \"https://data.kb.se/dataset/hwpjh1n7fnsskvfr\"\n",
    "\"@type\": \"Dataset\"\n",
    "\"title\": \"Norrköpings Weko-tidningar 1758-1786\"\n",
    "\n",
    "\"@id\": \"https://data.kb.se/dataset/hwpjh11gfnk12r2z\"\n",
    "\"@type\": \"Dataset\"\n",
    "\"title\": \"Svenska dagbladet\"\n",
    "\n",
    "\"@id\": \"https://data.kb.se/dataset/l0sqlrn8jcp5svln\"\n",
    "\"@type\": \"Dataset\"\n",
    "\"title\": \"Post och inrikes tidningar 1645-1705\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First experiments and results\n",
    "Results – could request all 10K dark-packages through the API and extract relevant information. This information was put into a dataframe and saved as aftonbladet.csv. The next steps uses this csv file instead of API calls to get started. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib.parse as ul\n",
    "import pandas as pd\n",
    "from hurry.filesize import size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sends a request to the API to get all dark-packages using a dataset id (for aftonbladet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.kb.se/search/?q=%2A&type=package&inDataset=https%3A//data.kb.se/dataset/6ld76n1p44s12ht1&searchGranularity=package&_sort=title&limit=20000&offset=0\n"
     ]
    }
   ],
   "source": [
    "headers = {'accept': 'application/json'}\n",
    "q = '*'\n",
    "#inDataset = 'https://data.kb.se/dataset/7mf87n2g5xzjf8r4' #aftonbladet\n",
    "#inDataset = 'https://data.kb.se/dataset/hwpjh11gfnk12r2z' #SvD\n",
    "inDataset = 'https://data.kb.se/dataset/6ld76n1p44s12ht1' #DN 1864-1893\n",
    "limit = 20000\n",
    "offset = 0\n",
    "url = f\"https://data.kb.se/search/?q={ul.quote(q)}&type=package&inDataset={ul.quote(inDataset)}&searchGranularity=package&_sort=title&limit={str(limit)}&offset={str(offset)}\"\n",
    "print(url)\n",
    "# This gets a dataset, but it is useless as the dataset contains no open references to dark-packages. These probably need to be scraped, incredibly...\n",
    "#url = \"https://data.kb.se/dataset/7mf87n2g5xzjf8r4\"\n",
    "response = requests.get(url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the http response. Now parse it and populate dark_list with dark_dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 5\n",
      "Length 'hits': 10000\n",
      "Added 10000 dark-packages to list.\n"
     ]
    }
   ],
   "source": [
    "json_response = json.loads(response.text)\n",
    "\n",
    "print(\"Length: \" + str(len(json_response)))\n",
    "\n",
    "l = json_response[\"hits\"]\n",
    "\n",
    "number_of_dark_packages = len(l)\n",
    "print(\"Length 'hits': \" + str(number_of_dark_packages))\n",
    "\n",
    "dark_dict = {\"id\": \"\", \n",
    "             \"id_url\": \"\",\n",
    "             \"title\": \"\",\n",
    "             \"date_published\": \"\"}\n",
    "dark_list = []\n",
    "ids = []\n",
    "id_urls = []\n",
    "titles = []\n",
    "dates_published = []\n",
    "\n",
    "\n",
    "# Loop\n",
    "for i in range(len(l)):\n",
    "    id = l[i][\"@id\"]\n",
    "    id_url = \"https://data.kb.se/\" + id\n",
    "    title = l[i][\"title\"]\n",
    "    date_published = l[i][\"datePublished\"]\n",
    "    \n",
    "    dark_dict['id']=id\n",
    "    dark_dict['id_url'] = id_url\n",
    "    dark_dict['date_published'] = date_published\n",
    "    dark_dict['title'] = title\n",
    "    \n",
    "    dark_list.append(dark_dict.copy())\n",
    "    \n",
    "    ids.append(id)\n",
    "    id_urls.append(id_url)\n",
    "    titles.append(title)\n",
    "    dates_published.append(date_published)\n",
    "    \n",
    "\n",
    "    # print(\"id = \"+id)\n",
    "    #print(\"id_url = \" + id_url)\n",
    "    # print(\"titel = \"+title)\n",
    "    # print(\"date_published = \"+date_published)\n",
    "\n",
    "#print(dark_list)\n",
    "count=len(dark_list)\n",
    "print(f\"Added {count} dark-packages to list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect which years are included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dark_list)\n",
    "\n",
    "# for i in range(len(dark_list)):\n",
    "#     print()\n",
    "print(dates_published)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parse dark_list and write a csv file. This is how we create the csv files used in the later notebooks for generating download scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file='aftonbladet.csv'\n",
    "#csv_file='svd.csv'\n",
    "csv_file = 'dn.csv'\n",
    "\n",
    "dict = {'id': ids, 'id_url': id_urls, 'date_published': dates_published, 'title': titles}\n",
    "df = pd.DataFrame(dict)\n",
    "\n",
    "df.head()\n",
    "df.to_csv(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Continue parsing from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some test and extract some info from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                   dark-3702356\n",
      "id_url            https://data.kb.se/dark-3702356\n",
      "date_published                1893-03-24 00:00:00\n",
      "title                  DAGENS NYHETER  1893-03-24\n",
      "Name: 9999, dtype: object\n",
      "dark-3702356\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dn.csv', index_col=0, parse_dates=['date_published'])\n",
    "df.head()\n",
    "df.tail()\n",
    "\n",
    "# pandas regonizes timestamp correctly\n",
    "last_row = df.iloc[number_of_dark_packages-1]\n",
    "print (last_row)\n",
    "print(last_row['id'])\n",
    "print (type(last_row['date_published'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some test on single row, then on one specific dark-package to find what information to extract and how. This code should then be rolled into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.kb.se/dark-3702356\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/bib13991099_18930324_0_8588b_0001.jp2\n",
      "filename: bib13991099_18930324_0_8588b_0001.jp2\n",
      "contentSize: 25M\n",
      "md5: dfc0093df9c1ec482c8dca0494164e00\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/bib13991099_18930324_0_8588b_0002.jp2\n",
      "filename: bib13991099_18930324_0_8588b_0002.jp2\n",
      "contentSize: 24M\n",
      "md5: 6c73b186d47401a9bf626c1fcfa9a237\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/bib13991099_18930324_0_8588b_0001_alto.xml\n",
      "filename: bib13991099_18930324_0_8588b_0001_alto.xml\n",
      "contentSize: 1M\n",
      "md5: 269dd5a062a82db5a15c9b702d29a01a\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/bib13991099_18930324_0_8588b_0002_alto.xml\n",
      "filename: bib13991099_18930324_0_8588b_0002_alto.xml\n",
      "contentSize: 1M\n",
      "md5: 8211b4f113d8b556253b1ff42c9f2153\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/bib13991099_18930324_0_8588b_performance.xml\n",
      "filename: bib13991099_18930324_0_8588b_performance.xml\n",
      "contentSize: 9K\n",
      "md5: 452d3a788695c835bed7b69e865020f1\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/bib13991099_18930324_0_8588b.mets.metadata\n",
      "filename: bib13991099_18930324_0_8588b.mets.metadata\n",
      "contentSize: 23K\n",
      "md5: 79a7d59cca99bb28b366985f6d4fee38\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/representation.jsonld\n",
      "filename: representation.jsonld\n",
      "contentSize: 1K\n",
      "md5: 8117d4f893f711c10a81094ade09b20b920a88fda8763e9b367fdd8b5f276441\n",
      "------ /CDHU/ -------\n",
      "fileurl: https://data.kb.se/dark-3702356/manifest.jsonld\n",
      "filename: manifest.jsonld\n",
      "contentSize: 2K\n",
      "md5: caf7506345ae0f1818ae5d97718694885dddbbd9be3778cc7d49459eb588e010\n"
     ]
    }
   ],
   "source": [
    "headers = {'accept': 'application/json'}\n",
    "dark_id = last_row['id']\n",
    "url = f'https://data.kb.se/{dark_id}'\n",
    "print(url)\n",
    "# Get dark-package:\n",
    "response = requests.get(url, headers=headers)\n",
    "#print(response.text)\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "#print(type(response))\n",
    "#print(type(json_response))\n",
    "#print(type(json_response['includes']))\n",
    "l = json_response['includes']\n",
    "\n",
    "# Explorative code that has been rolled into the loop/functions below\n",
    "# print(\"Length of 'includes': \"+str(len(l)))\n",
    "# fileurl = l[0]['@id']\n",
    "# filename = l[0]['fileName']\n",
    "# contentsize = l[0]['contentSize']\n",
    "# md5 = l[0]['checksum']['value']\n",
    "# print(f'------ /CDHU/ Single file (idx:0) request from {dark_id}: -------')\n",
    "# print(f'fileurl: {fileurl}')\n",
    "# print(f'filename: {filename}')\n",
    "# print(f'contentSize: {size(contentsize)}')\n",
    "# print(f'md5: {md5}')\n",
    "\n",
    "# This function gets items and data from dark-package:\n",
    "def get_dark_package_items (l,i):\n",
    "    fileurl = l[i]['@id']\n",
    "    filename = l[i]['fileName']\n",
    "    contentsize = l[i]['contentSize']\n",
    "    md5 = l[i]['checksum']['value']\n",
    "    return fileurl,filename,contentsize,md5\n",
    "\n",
    "def print_dark_package_items(l):\n",
    "    # This loop gets relevant items and metadata contained in the dark-package   \n",
    "    for i in range(len(l)):\n",
    "        fileurl, filename, contentsize, md5 = get_dark_package_items(l,i)\n",
    "        print(f'------ /CDHU/ -------')\n",
    "        print(f'fileurl: {fileurl}')\n",
    "        print(f'filename: {filename}')\n",
    "        print(f'contentSize: {size(contentsize)}')\n",
    "        print(f'md5: {md5}')\n",
    "        \n",
    "print_dark_package_items(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test some looping ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total issues (dark-packages): 10000\n",
      "Issues to parse: 10000\n",
      "(idx: 0 / title: DAGENS NYHETER  1864-12-23)https://data.kb.se/dark-3675680\n",
      "(idx: 1 / title: DAGENS NYHETER  1865-01-02)https://data.kb.se/dark-3675682\n",
      "(idx: 2 / title: DAGENS NYHETER  1865-01-03)https://data.kb.se/dark-3675683\n",
      "(idx: 3 / title: DAGENS NYHETER  1865-01-04)https://data.kb.se/dark-3675684\n",
      "(idx: 4 / title: DAGENS NYHETER  1865-01-05)https://data.kb.se/dark-3675685\n",
      "(idx: 5 / title: DAGENS NYHETER  1865-01-07)https://data.kb.se/dark-3675687\n",
      "(idx: 6 / title: DAGENS NYHETER  1865-01-09)https://data.kb.se/dark-3675686\n",
      "(idx: 7 / title: DAGENS NYHETER  1865-01-10)https://data.kb.se/dark-3675688\n",
      "(idx: 8 / title: DAGENS NYHETER  1865-01-11)https://data.kb.se/dark-3675689\n",
      "(idx: 9 / title: DAGENS NYHETER  1865-01-12)https://data.kb.se/dark-3675690\n",
      "Length of issue_list: 10\n",
      "https://data.kb.se/dark-3675680/bib13991099_18641223_0_1_0001.jp2\n",
      "https://data.kb.se/dark-3675680/bib13991099_18641223_0_1_0002.jp2\n",
      "https://data.kb.se/dark-3675680/bib13991099_18641223_0_1_0003.jp2\n",
      "https://data.kb.se/dark-3675680/bib13991099_18641223_0_1_0004.jp2\n",
      "https://data.kb.se/dark-3675680/bib13991099_18641223_0_1_0001_alto.xml\n"
     ]
    }
   ],
   "source": [
    "print(\"Total issues (dark-packages):\", len(df.index))\n",
    "length = len(df.index)\n",
    "LIMIT = 10\n",
    "length = LIMIT\n",
    "print(\"Issues to parse:\", len(df.index))\n",
    "\n",
    "issue_list = []\n",
    "for i in range(length):\n",
    "    row = df.iloc[i]\n",
    "    dark_id = row['id']\n",
    "    title = row['title']\n",
    "    # print(dark_id)\n",
    "    headers = {'accept': 'application/json'}\n",
    "    url = f'https://data.kb.se/{dark_id}'\n",
    "    print(f'(idx: {i} / title: {title})', end='')\n",
    "    print(url)\n",
    "    # Get dark-package:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # print(response.text)\n",
    "\n",
    "    json_response = json.loads(response.text)\n",
    "    l = json_response['includes']\n",
    "\n",
    "    item_dict = {'title': '',\n",
    "                 'fileurl': '',\n",
    "                 'filename': '',\n",
    "                 'contentSize': '',\n",
    "                 'md5': '',\n",
    "                 'dark_id': ''\n",
    "                 }\n",
    "\n",
    "    \n",
    "    issue = []\n",
    "    for i in range(len(l)):\n",
    "        \n",
    "        fileurl = l[i]['@id']\n",
    "        filename = l[i]['fileName']\n",
    "        contentsize = l[i]['contentSize']\n",
    "        md5 = l[i]['checksum']['value']\n",
    "\n",
    "        #print(f'------ /CDHU/ -------')\n",
    "        # print(f'\\t\\tfileurl: {fileurl}')\n",
    "        # print(f'\\t\\tfilename: {filename}')\n",
    "        # print(f'\\t\\tcontentSize: {size(contentsize)}')\n",
    "        # print(f'\\t\\tmd5: {md5}')\n",
    "\n",
    "        # put in dict:\n",
    "        item_dict = {'title': title,\n",
    "                     'fileurl': fileurl,\n",
    "                     'filename': filename,\n",
    "                     'contentSize': contentsize,\n",
    "                     'md5': md5,\n",
    "                     'dark_id': dark_id\n",
    "                     }\n",
    "        issue.append(item_dict.copy())\n",
    "        pass\n",
    "    #print(issue)\n",
    "    issue_list.append(issue.copy())\n",
    "    pass\n",
    "pass\n",
    "###\n",
    "# NEXT: HOW TO BEST REPRESENT THIS AS DATA IN THE NEXT STEP?\n",
    "###\n",
    "\n",
    "print(\"Length of issue_list:\",len(issue_list))\n",
    "print(issue_list[0][0]['fileurl'])\n",
    "print(issue_list[0][1]['fileurl'])\n",
    "print(issue_list[0][2]['fileurl'])\n",
    "print(issue_list[0][3]['fileurl'])\n",
    "print(issue_list[0][4]['fileurl'])\n",
    "\n",
    "# dark_dict = {\"id\": \"\",\n",
    "#              \"id_url\": \"\",\n",
    "#              \"title\": \"\",\n",
    "#              \"date_published\": \"\"}\n",
    "# dark_list = []\n",
    "# ids = []\n",
    "# id_urls = []\n",
    "# titles = []\n",
    "# dates_published = []\n",
    "\n",
    "#\n",
    "# !!REMEMBER CHECK OUT THIS: Convert alto to ocr txt! https://github.com/cneud/alto-ocr-text . DONE.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Webscraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f572c7b3fed79c41ddbbb5f745e4c0cdf0dde02f3679772a29f9dde99c9384c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
