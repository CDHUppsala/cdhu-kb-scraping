{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract government SOU dataset from csv data \n",
    "\n",
    "This notebook parses a previously created csv data-file (sou_data.csv) to generate a bash-script that will download SOU dataset from https://sou.kb.se. \n",
    "\n",
    "The resulting bash script uses wget to download while also giving the pdf-files sanitized names based on titles in the csv-file. The generated script also sorts them into a folder structure organized by year. A pre-generated example script can be found in the subfolder [run-csv/](https://github.com/CDHUppsala/cdhu-kb-scraping/tree/main/run-csv). You can either edit/execute this script directly or run this notebook to customize the download-script to your liking. \n",
    "\n",
    "\n",
    "```python\n",
    "CDHU=\"\"\"\\\n",
    " ____ ____ ____ ____ ________ \n",
    "||C |||D |||H |||U |||       \n",
    "||__|||__|||__|||__|||_______\n",
    "|/__\\|/__\\|/__\\|/__\\|/_______\n",
    "\n",
    " EXTRACT SOU / BEAUTIFUL SOUP\n",
    "\"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code: Matts L/CDHU\n",
    "# Requires: pandas\n",
    "import re\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  titel                                                pdf  \\\n",
      "0  1922:1 första serien  https://weburn.kb.se/sou/580/urn-nbn-se-kb-dig...   \n",
      "1                1922:1  https://weburn.kb.se/sou/190/urn-nbn-se-kb-dig...   \n",
      "2  1922:2 första serien  https://weburn.kb.se/sou/580/urn-nbn-se-kb-dig...   \n",
      "3                1922:2  https://weburn.kb.se/sou/190/urn-nbn-se-kb-dig...   \n",
      "4  1922:3 första serien  https://weburn.kb.se/sou/580/urn-nbn-se-kb-dig...   \n",
      "\n",
      "                                                 urn                sou-nr  \\\n",
      "0  http://urn.kb.se/resolve?urn=urn:nbn:se:kb:sou...  1922:1 första serien   \n",
      "1  http://urn.kb.se/resolve?urn=urn:nbn:se:kb:sou...                1922:1   \n",
      "2  http://urn.kb.se/resolve?urn=urn:nbn:se:kb:sou...  1922:2 första serien   \n",
      "3  http://urn.kb.se/resolve?urn=urn:nbn:se:kb:sou...                1922:2   \n",
      "4  http://urn.kb.se/resolve?urn=urn:nbn:se:kb:sou...  1922:3 första serien   \n",
      "\n",
      "                                          full_titel  \n",
      "0                    Skolkommissionens betänkande. 4  \n",
      "1  Några iakttagelser från 1921 års riksdagsmanna...  \n",
      "2  Betänkande och förslag till förändrade grunder...  \n",
      "3           Byggnadsarbetarsakkunnigas betänkande. 2  \n",
      "4  Städernas särskilda skyldigheter och rättighet...  \n",
      "++ Read sou_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename='sou_data.csv'\n",
    "try:\n",
    "    df = pd.read_csv(f'./{filename}', sep=',', index_col=False)\n",
    "    print(df.head())\n",
    "except Exception: \n",
    "  print('-- Error!'+str(Exception))\n",
    "else:\n",
    "  print(f'++ Read {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Wrote 6114 lines as \"wget_all_csv.sh\" (3267 deactivated lines)\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "deactivated=0\n",
    "DECADE=1920 #set sou-series starting decade\n",
    "DECADE_BLACKLIST=[]\n",
    "# Uncomment this and edit blacklist to exclude whole decades from the download script\n",
    "#DECADE_BLACKLIST=[1920,1930,1940,1950,1960,1980,1990]; \"\"\" <== blacklist all except 1970s \"\"\"\n",
    "DECADE_BLACKLIST=[1920,1930,1940,1990]; \"\"\"<== blacklist 1920-1940, 1990 \"\"\"\n",
    "\n",
    "# generate output and write to file\n",
    "# \n",
    "try:\n",
    "  with open(r'wget_all_csv.sh', 'w') as fp:\n",
    "    fp.write(\"#!/bin/bash\\n\")\n",
    "    for ind in df.index:\n",
    "          df_titel = df['titel'][ind]\n",
    "          result = r = re.search(r'(\\d\\d\\d\\d)(:)(\\d\\d?\\d?)(.*)', df_titel)\n",
    "          year=(result.group(1))\n",
    "          nr=(result.group(3).zfill(3))\n",
    "          extra=(result.group(4))\n",
    "          full_titel = df['full_titel'][ind]\n",
    "          full_titel = \"\".join([c for c in full_titel if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "          full_titel = full_titel.replace(\"  \", \"_\")\n",
    "          full_titel = full_titel.replace(\" \", \"_\")\n",
    "          full_titel = full_titel[:120]\n",
    "\n",
    "          # this code block produces a commented line indicating the start of a new decade (for easier editing)\n",
    "          if int(year)-DECADE == 10:\n",
    "            DECADE=DECADE+10 # inc 10 yrs\n",
    "            fp.write(\"# \"+str(DECADE)+\"\\n\")\n",
    "\n",
    "          # This code block checks if the current SOU belongs to a blacklisted decade or not\n",
    "          if DECADE_BLACKLIST:\n",
    "            for d in DECADE_BLACKLIST:\n",
    "              test = int(year)-int(year)%10 # modulo operation and subtraction floors year to get decade:\n",
    "              if test == d: \n",
    "                  DO_COMMENT=\"#\" # deactivate line by inserting a comment if year is blacklisted\n",
    "                  deactivated+=1\n",
    "                  break #break out of loop if blacklist match was found for year ...\n",
    "              else: #  ... if not, clear DO_COMMENT\n",
    "                DO_COMMENT = \"\"\n",
    "          \n",
    "          #c=1920\n",
    "\n",
    "          url = df['pdf'][ind]\n",
    "          id = df['sou-nr'][ind]\n",
    "          #sanitize\n",
    "          #id = \"\".join([c for c in id if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "          id = id.replace(\" \", \"_\")\n",
    "          id = id.replace(\":\", \"-\")\n",
    "          id = id.replace(\"/\", \"-\")\n",
    "          id = 'sou-'+id\n",
    "          longtitle = df['full_titel'][ind]\n",
    "\n",
    "          # set up download command and output filename and path\n",
    "          command = 'wget --continue -O \\\"' +year+'/'+'sou-'+year+'-'+nr+extra   \\\n",
    "                                            +'-'  \\\n",
    "                                            +full_titel   \\\n",
    "                                            +'.pdf\\\" '+url\n",
    "          # set up mkdir command to prefix wget\n",
    "          prefix='mkdir -p '+year+'/ && '\n",
    "          command=prefix+command\n",
    "          # deactivate line if blacklisted\n",
    "          command=DO_COMMENT+command\n",
    "          #write each item on a new line\n",
    "          fp.write(\"%s\\n\" % command)\n",
    "          count=count+1\n",
    "except:\n",
    "  print(\"-- Error!\")\n",
    "else:\n",
    "  print(f'++ Wrote {count} lines as \"wget_all_csv.sh\" ({deactivated} deactivated lines)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f572c7b3fed79c41ddbbb5f745e4c0cdf0dde02f3679772a29f9dde99c9384c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Webscraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
